# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

test_suite_config: # Overall configuration settings for the test suite
  examples: ["chatqna"]  # The specific test cases being tested, e.g., chatqna, codegen, codetrans, faqgen, audioqna, visualqna
  deployment_type: "k8s"  # Default is "k8s", can also be "docker"
  service_ip: None  # Leave as None for k8s, specify for Docker
  service_port: None  # Leave as None for k8s, specify for Docker
  warm_ups: 0  # Number of test requests for warm-up
  run_time: 60m  # The max total run time for the test suite
  seed:  # The seed for all RNGs
  user_queries: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]  # Number of test requests at each concurrency level
  query_timeout: 120  # Number of seconds to wait for a simulated user to complete any executing task before exiting. 120 sec by defeult.
  random_prompt: false  # Use random prompts if true, fixed prompts if false
  collect_service_metric: false  # Collect service metrics if true, do not collect service metrics if false
  data_visualization: false # Generate data visualization if true, do not generate data visualization if false
  llm_model: "Intel/neural-chat-7b-v3-3"  # The LLM model used for the test
  test_output_dir: "/home/sdp/benchmark_output"  # The directory to store the test output
  load_shape:              # Tenant concurrency pattern
    name: constant           # poisson or constant(locust default load shape)
    params:                  # Loadshape-specific parameters
      constant:                # Poisson load shape specific parameters, activate only if load_shape is poisson
        concurrent_level: 4      # If user_queries is specified, concurrent_level is target number of requests per user. If not, it is the number of simulated users
      poisson:                 # Poisson load shape specific parameters, activate only if load_shape is poisson
        arrival-rate: 1.0        # Request arrival rate

test_cases:
  chatqna:
    embedding:
      run_test: false
      service_name: "embedding-svc"  # Replace with your service name
    embedserve:
      run_test: false
      service_name: "embedding-dependency-svc"  # Replace with your service name
    retriever:
      run_test: false
      service_name: "retriever-svc"  # Replace with your service name
      parameters:
        search_type: "similarity"
        k: 4
        fetch_k: 20
        lambda_mult: 0.5
        score_threshold: 0.2
    reranking:
      run_test: false
      service_name: "reranking-svc"  # Replace with your service name
      parameters:
        top_n: 1
    rerankserve:
      run_test: false
      service_name: "reranking-dependency-svc"  # Replace with your service name
    llm:
      run_test: false
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: false
      service_name: "llm-dependency-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "chatqna-backend-server-svc"  # Replace with your service name
      service_list:  # Replace with your k8s service names if deploy with k8s
                     # or container names if deploy with Docker for metrics collection,
                     # activate if collect_service_metric is true
        - "chatqna-backend-server-svc"
        - "chatqna-nginx-svc"
        - "dataprep-svc"
        - "embedding-dependency-svc"
        - "llm-dependency-svc"
        - "reranking-dependency-svc"
        - "retriever-svc"
        - "vector-db"
      dataset: # Activate if random_prompt=true: leave blank = default dataset(WebQuestions) or sharegpt

  codegen:
    llm:
      run_test: true
      service_name: "llm-dependency-svc"  # Replace with your service name
      parameters:
        model_name: "Qwen/CodeQwen1.5-7B-Chat"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "codegen-backend-svc"  # Replace with your service name

  codetrans:
    llm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "HuggingFaceH4/mistral-7b-grok"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "codetrans-llm-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "codetrans-backend-server-svc"  # Replace with your service name

  faqgen:
    llm:
      run_test: false
      service_name: "faq-tgi-svc"  # Replace with your service name
      parameters:
        model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: false
      service_name: "faq-micro-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "faq-mega-server-svc"  # Replace with your service name

  audioqna:
    asr:
      run_test: true
      service_name: "asr-svc"  # Replace with your service name
    llm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "Intel/neural-chat-7b-v3-3"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
    tts:
      run_test: true
      service_name: "tts-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "audioqna-backend-server-svc"  # Replace with your service name

  visualqna:
    lvm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "llava-hf/llava-v1.6-mistral-7b-hf"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    lvmserve:
      run_test: true
      service_name: "lvm-serving-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "visualqna-backend-server-svc"  # Replace with your service name
