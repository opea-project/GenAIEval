# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

test_suite_config: # Overall configuration settings for the test suite
  examples: ["chatqna"]  # The specific test cases being tested, e.g., chatqna, codegen, codetrans, faqgen, audioqna, visualqna
  deployment_type: "k8s"  # Default is "k8s", can also be "docker"
  service_ip: None  # Leave as None for k8s, specify for Docker
  service_port: None  # Leave as None for k8s, specify for Docker
  concurrent_level: 4  # The concurrency level, adjustable based on requirements
  user_queries: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]  # Number of test requests at each concurrency level
  random_prompt: false  # Use random prompts if true, fixed prompts if false
  run_time: 60m  # The max total run time for the test suite
  collect_service_metric: false  # Collect service metrics if true, do not collect service metrics if false
  data_visualization: false # Generate data visualization if true, do not generate data visualization if false
  llm_model: "Intel/neural-chat-7b-v3-3"  # The LLM model used for the test
  test_output_dir: "/home/sdp/benchmark_output"  # The directory to store the test output

test_cases:
  chatqna:
    embedding:
      run_test: false
      service_name: "embedding-svc"  # Replace with your service name
    embedserve:
      run_test: false
      service_name: "embedding-dependency-svc"  # Replace with your service name
    retriever:
      run_test: false
      service_name: "retriever-svc"  # Replace with your service name
      parameters:
        search_type: "similarity"
        k: 4
        fetch_k: 20
        lambda_mult: 0.5
        score_threshold: 0.2
    reranking:
      run_test: false
      service_name: "reranking-svc"  # Replace with your service name
      parameters:
        top_n: 1
    rerankserve:
      run_test: false
      service_name: "reranking-dependency-svc"  # Replace with your service name
    llm:
      run_test: false
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: false
      service_name: "llm-dependency-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "chatqna-backend-server-svc"  # Replace with your service name
      service_list:  # Replace with your k8s service names for metrics collection,
                     # activate if deployment_type is k8s and collect_service_metric is true
        - "chatqna-tei"
        - "chatqna-teirerank"

  codegen:
    llm:
      run_test: true
      service_name: "llm-dependency-svc"  # Replace with your service name
      parameters:
        model_name: "Qwen/CodeQwen1.5-7B-Chat"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "codegen-backend-svc"  # Replace with your service name

  codetrans:
    llm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "HuggingFaceH4/mistral-7b-grok"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "codetrans-llm-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "codetrans-backend-server-svc"  # Replace with your service name

  faqgen:
    llm:
      run_test: false
      service_name: "faq-tgi-svc"  # Replace with your service name
      parameters:
        model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llm_serving:
      run_test: false
      service_name: "faq-micro-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "faq-mega-server-svc"  # Replace with your service name

  audioqna:
    asr:
      run_test: true
      service_name: "asr-svc"  # Replace with your service name
    llm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "Intel/neural-chat-7b-v3-3"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
    tts:
      run_test: true
      service_name: "tts-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "audioqna-backend-server-svc"  # Replace with your service name

  visualqna:
    lvm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "llava-hf/llava-v1.6-mistral-7b-hf"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    lvmserve:
      run_test: true
      service_name: "lvm-serving-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "visualqna-backend-server-svc"  # Replace with your service name
