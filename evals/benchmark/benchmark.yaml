# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

test_suite_config: # Overall configuration settings for the test suite
  examples: ["chatqna"]  # The specific test cases being tested, e.g., chatqna, codegen, codetrans, faqgen, audioqna, visualqna
  deployment_type: "k8s"  # Default is "k8s", can also be "docker"
  service_ip: None  # Leave as None for k8s, specify for Docker
  service_port: None  # Leave as None for k8s, specify for Docker
  warm_ups: 0  # Number of test requests for warm-up
  run_time: 60m  # The max total run time for the test suite
  seed:  # The seed for all RNGs
  user_queries: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]  # Number of test requests at each concurrency level
  query_timeout: 120  # Number of seconds to wait for a simulated user to complete any executing task before exiting. 120 sec by defeult.
  random_prompt: false  # Use random prompts if true, fixed prompts if false
  collect_service_metric: false  # Collect service metrics if true, do not collect service metrics if false
  data_visualization: false # Generate data visualization if true, do not generate data visualization if false
  llm_model: "Intel/neural-chat-7b-v3-3"  # The LLM model used for the test
  test_output_dir: "/home/sdp/benchmark_output"  # The directory to store the test output
  load_shape:              # Tenant concurrency pattern
    name: constant           # poisson or constant(locust default load shape)
    params:                  # Loadshape-specific parameters
      constant:                # Constant load shape specific parameters, activate only if load_shape.name is constant
        concurrent_level: 4      # If user_queries is specified, concurrent_level is target number of requests per user. If not, it is the number of simulated users
        # arrival_rate: 1.0       # Request arrival rate. If set, concurrent_level will be overridden, constant load will be generated based on arrival-rate
      poisson:                 # Poisson load shape specific parameters, activate only if load_shape.name is poisson
        arrival_rate: 1.0        # Request arrival rate
  namespace: "" # Fill the user-defined namespace. Otherwise, it will be default.

test_cases:
  chatqna:
    embedding:
      run_test: false
      service_name: "embedding-svc"  # Replace with your service name
    embedserve:
      run_test: false
      service_name: "embedding-dependency-svc"  # Replace with your service name
    retriever:
      run_test: false
      service_name: "retriever-svc"  # Replace with your service name
      parameters:
        search_type: "similarity"
        k: 4
        fetch_k: 20
        lambda_mult: 0.5
        score_threshold: 0.2
    reranking:
      run_test: false
      service_name: "reranking-svc"  # Replace with your service name
      parameters:
        top_n: 1
    rerankserve:
      run_test: false
      service_name: "reranking-dependency-svc"  # Replace with your service name
    llm:
      run_test: false
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: false
      service_name: "llm-dependency-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "chatqna-backend-server-svc"  # Replace with your service name
      service_list:  # Replace with your k8s service names if deploy with k8s
                     # or container names if deploy with Docker for metrics collection,
                     # activate if collect_service_metric is true
        - "chatqna-backend-server-svc"
        - "chatqna-nginx-svc"
        - "dataprep-svc"
        - "embedding-dependency-svc"
        - "llm-dependency-svc"
        - "reranking-dependency-svc"
        - "retriever-svc"
        - "vector-db"
      dataset:  # Activate if random_prompt=true: leave blank = default dataset(WebQuestions) or sharegpt
      prompts: In an increasingly complex world where technology has rapidly advanced and evolved far beyond our wildest dreams, humanity now stands on the precipice of a revolutionary new era that is filled with endless possibilities, profound and significant changes, as well as intricate challenges that we must actively address. The year is now 2050, and artificial intelligence has seamlessly woven itself deeply and intricately into the very fabric of everyday life. Autonomous vehicles glide effortlessly and smoothly through the bustling, vibrant, and lively city streets, while drones swiftly and accurately deliver packages with pinpoint precision, making logistics and delivery systems more efficient, streamlined, and advanced than ever before in the entire history of humankind and technological development. Smart homes, equipped with cutting-edge advanced sensors and sophisticated algorithms, anticipate every possible need and requirement of their inhabitants, creating an environment of unparalleled convenience, exceptional comfort, and remarkable efficiency that enhances our daily lives. However, with these remarkable and groundbreaking advancements come a host of new challenges, uncertainties, and ethical dilemmas that society must confront, navigate, and address in a thoughtful and deliberate manner. As we carefully navigate through this brave new world filled with astonishing technological marvels, innovations, and breakthroughs, questions about the implications and consequences of AI technologies become increasingly pressing, relevant, and urgent for individuals and communities alike. Issues surrounding privacy—how our personal data is collected, securely stored, processed, and utilized—emerge alongside significant concerns about security in a rapidly evolving digital landscape where vulnerabilities can be easily and readily exploited by malicious actors, hackers, and cybercriminals. Moreover, philosophical inquiries regarding the very nature of consciousness itself rise prominently to the forefront of public discourse, debate, and discussion, inviting diverse perspectives, opinions, and ethical considerations from various stakeholders. In light of these profound developments and transformative changes that we are witnessing, I would like to gain a much deeper, broader, and more comprehensive understanding of what artificial intelligence truly is and what it encompasses in its entirety and complexity. Could you elaborate extensively, thoroughly, and comprehensively on its precise definition, its wide-ranging and expansive scope, as well as the myriad and diverse ways it significantly impacts our daily lives, personal experiences, and society as a whole in various dimensions and aspects? # User-customized prompts, activate if random_prompt=false.
      max_output: 128  # max number of output tokens

  codegen:
    llm:
      run_test: true
      service_name: "llm-dependency-svc"  # Replace with your service name
      parameters:
        model_name: "Qwen/CodeQwen1.5-7B-Chat"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "codegen-backend-svc"  # Replace with your service name

  codetrans:
    llm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "HuggingFaceH4/mistral-7b-grok"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "codetrans-llm-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "codetrans-backend-server-svc"  # Replace with your service name

  faqgen:
    llm:
      run_test: false
      service_name: "faq-tgi-svc"  # Replace with your service name
      parameters:
        model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: false
      service_name: "faq-micro-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "faq-mega-server-svc"  # Replace with your service name

  audioqna:
    asr:
      run_test: true
      service_name: "asr-svc"  # Replace with your service name
    llm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "Intel/neural-chat-7b-v3-3"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    llmserve:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
    tts:
      run_test: true
      service_name: "tts-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "audioqna-backend-server-svc"  # Replace with your service name

  visualqna:
    lvm:
      run_test: true
      service_name: "llm-svc"  # Replace with your service name
      parameters:
        model_name: "llava-hf/llava-v1.6-mistral-7b-hf"
        max_new_tokens: 128
        temperature: 0.01
        top_k: 10
        top_p: 0.95
        repetition_penalty: 1.03
        streaming: true
    lvmserve:
      run_test: true
      service_name: "lvm-serving-svc"  # Replace with your service name
    e2e:
      run_test: true
      service_name: "visualqna-backend-server-svc"  # Replace with your service name
